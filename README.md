# Versor: Geometric Transformer for Conformal Geometric Algebra

A native Geometric Algebra Transformer architecture based on Conformal Geometric Algebra Cl(4,1), implementing multivector representations and geometric products for deep learning.

## ğŸš€ Quick Start

**New to Versor?** Start with the interactive tutorial:

```bash
jupyter notebook quickstart.ipynb
```

This notebook walks you through:
1. Importing the Versor architecture
2. Creating a simple dataset (learning xÂ²)
3. Training the model
4. Testing and evaluation

The quickstart provides a minimal working example you can adapt to your own problems!

## ğŸ“ Repository Structure

```
Versor/
â”œâ”€â”€ quickstart.ipynb          # ğŸ‘ˆ START HERE! Interactive tutorial
â”œâ”€â”€ Model/                    # Core Versor architecture
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ core.py              # Geometric algebra operations (Cl(4,1))
â”‚   â”œâ”€â”€ layers.py            # VersorLinear, VersorAttention
â”‚   â””â”€â”€ model.py             # VersorTransformer, VersorBlock
â”œâ”€â”€ tasks/                    # Task-specific implementations
â”‚   â”œâ”€â”€ nlp/                 # Natural language processing tasks
â”‚   â”œâ”€â”€ vision/              # Computer vision tasks
â”‚   â”œâ”€â”€ nbody/               # N-body physics simulations
â”‚   â”œâ”€â”€ topology/            # Topological reasoning tasks
â”‚   â”œâ”€â”€ multimodal/          # Multimodal learning
â”‚   â”œâ”€â”€ scripts/             # Analysis and benchmarking scripts
â”‚   â””â”€â”€ figures/             # Generated plots and visualizations
â”œâ”€â”€ library/                  # Utility functions and helpers
â”œâ”€â”€ gatr/                     # GATr baseline implementation
â”œâ”€â”€ data/                     # Datasets
â”œâ”€â”€ results/                  # Experimental results
â”œâ”€â”€ Paper/                    # Research paper and figures
â”œâ”€â”€ requirements.txt          # Python dependencies
â””â”€â”€ kernel.py                # Custom CUDA kernels
```

## ğŸ”§ Installation

1. Clone the repository:
```bash
git clone <repository-url>
cd Versor
```

2. Install dependencies:
```bash
pip install -r requirements.txt
```

3. (Optional) For CUDA acceleration:
```bash
# Ensure you have CUDA toolkit installed
# The custom kernels in kernel.py will be compiled automatically
```

## ğŸ“š Core Components

### Model Architecture

- **VersorTransformer**: Full geometric transformer for Cl(4,1)
- **VersorBlock**: High-performance block with Geometric Product Attention (GPA)
- **VersorAttention**: Attention mechanism using geometric products
- **VersorLinear**: Linear layer preserving multivector structure
- **RecursiveRotorAccumulator**: Rotor-based sequence pooling

### Geometric Algebra Operations

- `conformal_lift`: Lift 4D points to Cl(4,1) multivectors
- `gp_cl41`: Geometric product in Cl(4,1)
- `wedge_cl41`: Wedge (exterior) product
- `inner_cl41`: Inner product
- `reverse_cl41`: Clifford conjugation
- `normalize_cl41`: Manifold normalization

## ğŸ§ª Example Tasks

### NLP: Dyck Language Recognition
```bash
cd tasks/nlp
python dyck_rotor.py --depths 20 50 100 --repeats 3
```

### Vision: CIFAR Classification
```bash
cd tasks/vision
# See task-specific README for details
```

### Physics: N-Body Simulation
```bash
cd tasks/nbody
# See task-specific README for details
```

## ğŸ“Š Benchmarking

Run comprehensive benchmarks:

```bash
# Small-scale benchmark
python tasks/scripts/benchmark_versor_small.py

# Large-scale benchmark
python tasks/scripts/benchmark_versor_large.py

# Compare with GATr baseline
python tasks/scripts/benchmark_gatr.py

# Generate scaling plots
python tasks/scripts/generate_plot.py
```

## ğŸ¯ Adapting to Your Problem

The `quickstart.ipynb` provides a template for:
- **Regression tasks**: Predict continuous values
- **Classification tasks**: Modify the output layer
- **Sequence modeling**: Use the rotor accumulator
- **Custom data**: Adapt the data loading section

Key steps:
1. Prepare your data in the appropriate format
2. Lift data to Cl(4,1) using `conformal_lift` or custom embedding
3. Configure model hyperparameters (embed_dim, n_heads, n_layers)
4. Train with standard PyTorch training loop
5. Evaluate and visualize results

## ğŸ“– Citation

If you use this code in your research, please cite:

```bibtex
# See CITATION.cff for full citation details
```

## ğŸ¤ Contributing

Contributions are welcome! Please:
1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Submit a pull request

## ğŸ“„ License

See LICENSE file for details.

## ğŸ”— Resources

- **Paper**: See `Paper/main.tex` for the full research paper
- **Documentation**: Each task folder contains specific documentation
- **Examples**: Browse `tasks/` for domain-specific implementations

## âš¡ Performance Tips

1. **CUDA**: Enable TF32 for faster training on Ampere+ GPUs
2. **Compilation**: Use `torch.compile()` for 2-5x speedup (PyTorch 2.0+)
3. **Batch Size**: Larger batches improve GPU utilization
4. **Mixed Precision**: Consider using automatic mixed precision (AMP)

## ğŸ› Troubleshooting

**Import errors**: Ensure you're in the repository root and have installed all dependencies

**CUDA errors**: Check CUDA compatibility with your PyTorch version

**Memory issues**: Reduce batch size or model dimensions

**Convergence issues**: Try adjusting learning rate or using gradient clipping

## ğŸ“§ Contact

For questions or issues, please open a GitHub issue or contact the authors.

---

**Happy Geometric Learning! ğŸ“âœ¨**
