{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Versor Transformer - Quick Start Guide\n",
                "\n",
                "This notebook provides a minimal working example to get you started with the Versor architecture.\n",
                "\n",
                "The Versor Transformer is a geometric deep learning architecture based on Conformal Geometric Algebra (Cl(4,1)).\n",
                "It uses multivector representations and geometric products for learning tasks.\n",
                "\n",
                "## What you'll learn:\n",
                "1. How to import the Versor architecture\n",
                "2. How to create a simple dataset (learning x²)\n",
                "3. How to train the model\n",
                "4. How to test and evaluate performance\n",
                "\n",
                "Feel free to adapt this code to your own problems!"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Cell 1: Import the Versor Architecture"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "from Model import (\n",
                "    VersorTransformer,\n",
                "    conformal_lift,\n",
                "    normalize_cl41\n",
                ")\n",
                "\n",
                "# Set device\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f\"Using device: {device}\")\n",
                "\n",
                "# Set random seed for reproducibility\n",
                "torch.manual_seed(42)\n",
                "np.random.seed(42)\n",
                "\n",
                "print(\"✓ Versor architecture imported successfully!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Cell 2: Generate Random Train/Test Set (Learning x²)\n",
                "\n",
                "We'll create a simple regression task: learning the function f(x) = x²\n",
                "\n",
                "The data will be lifted to conformal geometric algebra space using the `conformal_lift` function."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def generate_quadratic_dataset(n_samples=1000, seq_len=10, x_range=(-2, 2)):\n",
                "    \"\"\"\n",
                "    Generate a dataset for learning x².\n",
                "    \n",
                "    Args:\n",
                "        n_samples: Number of samples to generate\n",
                "        seq_len: Sequence length (number of points per sample)\n",
                "        x_range: Range of x values\n",
                "    \n",
                "    Returns:\n",
                "        X: Input sequences (n_samples, seq_len, 4) - lifted to 4D space\n",
                "        y: Target values (n_samples,) - the quadratic values\n",
                "    \"\"\"\n",
                "    # Generate random x values\n",
                "    x = np.random.uniform(x_range[0], x_range[1], (n_samples, seq_len))\n",
                "    \n",
                "    # Compute y = x²\n",
                "    y = x[:, -1] ** 2  # Use the last value in sequence as target\n",
                "    \n",
                "    # Create 4D points for conformal lifting\n",
                "    # We'll use [x, 0, 0, 0] as our 4D representation\n",
                "    X_4d = np.zeros((n_samples, seq_len, 4))\n",
                "    X_4d[:, :, 0] = x  # First dimension contains our x values\n",
                "    \n",
                "    # Convert to tensors\n",
                "    X = torch.tensor(X_4d, dtype=torch.float32)\n",
                "    y = torch.tensor(y, dtype=torch.float32)\n",
                "    \n",
                "    return X, y\n",
                "\n",
                "# Generate datasets\n",
                "n_train = 800\n",
                "n_test = 200\n",
                "seq_len = 10\n",
                "\n",
                "X_train, y_train = generate_quadratic_dataset(n_train, seq_len)\n",
                "X_test, y_test = generate_quadratic_dataset(n_test, seq_len)\n",
                "\n",
                "# Move to device\n",
                "X_train, y_train = X_train.to(device), y_train.to(device)\n",
                "X_test, y_test = X_test.to(device), y_test.to(device)\n",
                "\n",
                "print(f\"✓ Dataset generated!\")\n",
                "print(f\"  Train: {X_train.shape}, Test: {X_test.shape}\")\n",
                "print(f\"  Sample input range: [{X_train[:, :, 0].min():.2f}, {X_train[:, :, 0].max():.2f}]\")\n",
                "print(f\"  Sample output range: [{y_train.min():.2f}, {y_train.max():.2f}]\")\n",
                "\n",
                "# Visualize a few samples\n",
                "plt.figure(figsize=(10, 4))\n",
                "for i in range(5):\n",
                "    x_vals = X_train[i, :, 0].cpu().numpy()\n",
                "    plt.scatter(x_vals, x_vals**2, alpha=0.6, label=f'Sample {i+1}')\n",
                "plt.xlabel('x')\n",
                "plt.ylabel('x²')\n",
                "plt.title('Sample Training Data (x²)')\n",
                "plt.legend()\n",
                "plt.grid(True, alpha=0.3)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Cell 3: Train the Architecture\n",
                "\n",
                "Now we'll create and train a Versor Transformer model.\n",
                "\n",
                "The model will learn to predict x² from sequences of x values."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Model hyperparameters\n",
                "embed_dim = 16      # Embedding dimension (number of multivector channels)\n",
                "n_heads = 4         # Number of attention heads\n",
                "n_layers = 2        # Number of transformer layers\n",
                "n_classes = 1       # Regression task (single output)\n",
                "expansion = 2       # MLP expansion factor\n",
                "\n",
                "# Create model\n",
                "model = VersorTransformer(\n",
                "    embed_dim=embed_dim,\n",
                "    n_heads=n_heads,\n",
                "    n_layers=n_layers,\n",
                "    n_classes=n_classes,\n",
                "    expansion=expansion,\n",
                "    use_rotor_pool=True\n",
                ").to(device)\n",
                "\n",
                "# Add an input embedding layer to lift data to multivector space\n",
                "class VersorRegressionModel(nn.Module):\n",
                "    def __init__(self, versor_model, seq_len, embed_dim):\n",
                "        super().__init__()\n",
                "        self.input_proj = nn.Linear(4, embed_dim * 32)  # Project 4D to multivector space\n",
                "        self.versor = versor_model\n",
                "        \n",
                "    def forward(self, x):\n",
                "        # x: (batch, seq_len, 4)\n",
                "        batch_size, seq_len, _ = x.shape\n",
                "        \n",
                "        # Project to multivector space\n",
                "        x = self.input_proj(x)  # (batch, seq_len, embed_dim * 32)\n",
                "        x = x.view(batch_size, seq_len, embed_dim, 32)  # (batch, seq_len, embed_dim, 32)\n",
                "        \n",
                "        # Normalize in multivector space\n",
                "        x = normalize_cl41(x)\n",
                "        \n",
                "        # Pass through Versor Transformer\n",
                "        out = self.versor(x)  # (batch, 1)\n",
                "        \n",
                "        return out.squeeze(-1)  # (batch,)\n",
                "\n",
                "# Wrap the model\n",
                "full_model = VersorRegressionModel(model, seq_len, embed_dim).to(device)\n",
                "\n",
                "# Count parameters\n",
                "n_params = sum(p.numel() for p in full_model.parameters() if p.requires_grad)\n",
                "print(f\"✓ Model created with {n_params:,} parameters\")\n",
                "\n",
                "# Training setup\n",
                "criterion = nn.MSELoss()\n",
                "optimizer = optim.AdamW(full_model.parameters(), lr=0.001, weight_decay=1e-4)\n",
                "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10)\n",
                "\n",
                "# Training loop\n",
                "n_epochs = 100\n",
                "batch_size = 32\n",
                "train_losses = []\n",
                "test_losses = []\n",
                "\n",
                "print(\"\\nStarting training...\")\n",
                "print(\"-\" * 60)\n",
                "\n",
                "for epoch in range(n_epochs):\n",
                "    # Training\n",
                "    full_model.train()\n",
                "    epoch_loss = 0\n",
                "    n_batches = 0\n",
                "    \n",
                "    # Mini-batch training\n",
                "    perm = torch.randperm(X_train.size(0))\n",
                "    for i in range(0, X_train.size(0), batch_size):\n",
                "        idx = perm[i:i+batch_size]\n",
                "        batch_X = X_train[idx]\n",
                "        batch_y = y_train[idx]\n",
                "        \n",
                "        optimizer.zero_grad()\n",
                "        pred = full_model(batch_X)\n",
                "        loss = criterion(pred, batch_y)\n",
                "        loss.backward()\n",
                "        \n",
                "        # Gradient clipping for stability\n",
                "        torch.nn.utils.clip_grad_norm_(full_model.parameters(), 1.0)\n",
                "        \n",
                "        optimizer.step()\n",
                "        \n",
                "        epoch_loss += loss.item()\n",
                "        n_batches += 1\n",
                "    \n",
                "    avg_train_loss = epoch_loss / n_batches\n",
                "    train_losses.append(avg_train_loss)\n",
                "    \n",
                "    # Evaluation\n",
                "    full_model.eval()\n",
                "    with torch.no_grad():\n",
                "        test_pred = full_model(X_test)\n",
                "        test_loss = criterion(test_pred, y_test).item()\n",
                "        test_losses.append(test_loss)\n",
                "    \n",
                "    # Learning rate scheduling\n",
                "    scheduler.step(test_loss)\n",
                "    \n",
                "    # Print progress\n",
                "    if (epoch + 1) % 10 == 0 or epoch == 0:\n",
                "        lr = optimizer.param_groups[0]['lr']\n",
                "        print(f\"Epoch {epoch+1:3d}/{n_epochs} | Train Loss: {avg_train_loss:.6f} | Test Loss: {test_loss:.6f} | LR: {lr:.6f}\")\n",
                "    \n",
                "    # Early stopping\n",
                "    if test_loss < 0.001:\n",
                "        print(f\"\\n✓ Converged at epoch {epoch+1}!\")\n",
                "        break\n",
                "\n",
                "print(\"-\" * 60)\n",
                "print(\"✓ Training complete!\")\n",
                "\n",
                "# Plot training curves\n",
                "plt.figure(figsize=(10, 4))\n",
                "plt.plot(train_losses, label='Train Loss', alpha=0.7)\n",
                "plt.plot(test_losses, label='Test Loss', alpha=0.7)\n",
                "plt.xlabel('Epoch')\n",
                "plt.ylabel('MSE Loss')\n",
                "plt.title('Training Progress')\n",
                "plt.legend()\n",
                "plt.grid(True, alpha=0.3)\n",
                "plt.yscale('log')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Cell 4: Test the Architecture\n",
                "\n",
                "Let's evaluate the trained model and visualize its predictions."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Evaluate on test set\n",
                "full_model.eval()\n",
                "with torch.no_grad():\n",
                "    test_pred = full_model(X_test)\n",
                "    test_loss = criterion(test_pred, y_test).item()\n",
                "    \n",
                "    # Calculate metrics\n",
                "    mae = torch.abs(test_pred - y_test).mean().item()\n",
                "    rmse = torch.sqrt(torch.mean((test_pred - y_test)**2)).item()\n",
                "    \n",
                "    # R² score\n",
                "    ss_res = torch.sum((y_test - test_pred)**2)\n",
                "    ss_tot = torch.sum((y_test - y_test.mean())**2)\n",
                "    r2 = 1 - (ss_res / ss_tot)\n",
                "\n",
                "print(\"=\"*60)\n",
                "print(\"EVALUATION RESULTS\")\n",
                "print(\"=\"*60)\n",
                "print(f\"Test MSE:  {test_loss:.6f}\")\n",
                "print(f\"Test MAE:  {mae:.6f}\")\n",
                "print(f\"Test RMSE: {rmse:.6f}\")\n",
                "print(f\"R² Score:  {r2:.6f}\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "# Visualize predictions\n",
                "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "# Plot 1: Predicted vs Actual\n",
                "y_test_np = y_test.cpu().numpy()\n",
                "test_pred_np = test_pred.cpu().numpy()\n",
                "\n",
                "axes[0].scatter(y_test_np, test_pred_np, alpha=0.5)\n",
                "axes[0].plot([y_test_np.min(), y_test_np.max()], \n",
                "             [y_test_np.min(), y_test_np.max()], \n",
                "             'r--', linewidth=2, label='Perfect Prediction')\n",
                "axes[0].set_xlabel('Actual x²')\n",
                "axes[0].set_ylabel('Predicted x²')\n",
                "axes[0].set_title('Predicted vs Actual Values')\n",
                "axes[0].legend()\n",
                "axes[0].grid(True, alpha=0.3)\n",
                "\n",
                "# Plot 2: Error distribution\n",
                "errors = test_pred_np - y_test_np\n",
                "axes[1].hist(errors, bins=30, alpha=0.7, edgecolor='black')\n",
                "axes[1].axvline(0, color='r', linestyle='--', linewidth=2, label='Zero Error')\n",
                "axes[1].set_xlabel('Prediction Error')\n",
                "axes[1].set_ylabel('Frequency')\n",
                "axes[1].set_title('Error Distribution')\n",
                "axes[1].legend()\n",
                "axes[1].grid(True, alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "# Show some example predictions\n",
                "print(\"\\nSample Predictions:\")\n",
                "print(\"-\" * 60)\n",
                "print(f\"{'Input x':<15} {'Actual x²':<15} {'Predicted x²':<15} {'Error':<15}\")\n",
                "print(\"-\" * 60)\n",
                "for i in range(min(10, len(X_test))):\n",
                "    x_val = X_test[i, -1, 0].item()  # Last x value in sequence\n",
                "    actual = y_test[i].item()\n",
                "    pred = test_pred[i].item()\n",
                "    error = pred - actual\n",
                "    print(f\"{x_val:<15.4f} {actual:<15.4f} {pred:<15.4f} {error:<15.4f}\")\n",
                "print(\"-\" * 60)\n",
                "\n",
                "print(\"\\n✓ Testing complete!\")\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"NEXT STEPS:\")\n",
                "print(\"=\"*60)\n",
                "print(\"1. Explore the tasks/ folder for more complex examples\")\n",
                "print(\"2. Try different architectures by adjusting hyperparameters\")\n",
                "print(\"3. Adapt this code to your own regression/classification problems\")\n",
                "print(\"4. Check out the paper in Paper/ for theoretical background\")\n",
                "print(\"=\"*60)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}